{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation: SegFormer finetuned on SceneParse150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ColorJitter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from huggingface_hub import cached_download, hf_hub_url\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    SegformerForSemanticSegmentation,\n",
    "    MobileViTForSemanticSegmentation,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import optuna\n",
    "\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /Users/mmenendezg/Developer/Projects/fluorescent-neuronal-cells/.venv/bin/huggingface-cli: bad interpreter: /Users/mmenendezg/Developer/Projects/semantic-segmentation/.venv/bin/python: no such file or directory\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Define the accelerator\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps:0\")\n",
    "    ACCELERATOR = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    ACCELERATOR = \"gpu\"\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    ACCELERATOR = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SceneParse150 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"scene_parse_150\", split=\"train[:500]\")\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_ds = dataset[\"train\"]\n",
    "test_ds = dataset[\"test\"]\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmenendezg/Developer/Projects/fluorescent-neuronal-cells/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:649: FutureWarning: 'cached_download' is the legacy way to download files from the HF hub, please consider upgrading to 'hf_hub_download'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"huggingface/label-files\"\n",
    "filename = \"ade20k-id2label.json\"\n",
    "id2label = json.load(\n",
    "    open(cached_download(hf_hub_url(repo_id, filename, repo_type=\"dataset\")), \"r\")\n",
    ")\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/Users/mmenendezg/Developer/Projects/fluorescent-neuronal-cells/.venv/lib/python3.11/site-packages/transformers/models/segformer/image_processing_segformer.py:101: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    MODEL_CHECKPOINT, reduce_labels=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n",
    "\n",
    "def train_transforms(batch):\n",
    "    images = [jitter(image.convert(\"RGB\")) for image in batch[\"image\"]]\n",
    "    labels = [mask for mask in batch[\"annotation\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(batch):\n",
    "    images = [img.convert(\"RGB\") for img in batch[\"image\"]]\n",
    "    labels = [mask for mask in batch[\"annotation\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"scene_parse_150\", split=\"train[:100]\")\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_sample_ds = dataset[\"train\"]\n",
    "test_sample_ds = dataset[\"test\"]\n",
    "del dataset\n",
    "\n",
    "train_sample_ds.set_transform(train_transforms)\n",
    "test_sample_ds.set_transform(val_transforms)\n",
    "\n",
    "train_sample_dataloader = DataLoader(train_sample_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_sample_dataloader = DataLoader(test_sample_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormer(pl.LightningModule):\n",
    "    def __init__(self, learning_rate, weight_decay):\n",
    "        super().__init__()\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            MODEL_CHECKPOINT, id2label=id2label, label2id=label2id\n",
    "        )\n",
    "        self.metric = evaluate.load(\"mean_iou\")\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        return self.model(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        return loss, logits\n",
    "\n",
    "    def compute_metric(self, logits, labels):\n",
    "        logits_tensor = nn.functional.interpolate(\n",
    "            logits,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = self.metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Calculate and log the loss\n",
    "        loss, logits = self.common_step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        # Calculate and log the metrics\n",
    "        metrics = self.compute_metric(logits, labels)\n",
    "        metrics = {\n",
    "            key: np.float32(value) for key, value in metrics.items()\n",
    "        }\n",
    "        # for k, v in metrics.items():\n",
    "        #     self.log(f\"train_{k}\", v.item())\n",
    "        self.log(\"train_mean_iou\", metrics[\"mean_iou\"])\n",
    "        self.log(\"train_mean_accuracy\", metrics[\"mean_accuracy\"])\n",
    "        self.log(\"train_overall_accuracy\", metrics[\"overall_accuracy\"])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Calculate and log the loss\n",
    "        loss, logits = self.common_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "        # Calculate and log the metrics\n",
    "        metrics = self.compute_metric(logits, labels)\n",
    "        metrics = {\n",
    "            key: np.float32(value) for key, value in metrics.items()\n",
    "        }\n",
    "        # for k, v in metrics.items():\n",
    "        #     self.log(f\"val_{k}\", v.item())\n",
    "        self.log(\"val_mean_iou\", metrics[\"mean_iou\"])\n",
    "        self.log(\"val_mean_accuracy\", metrics[\"mean_accuracy\"])\n",
    "        self.log(\"val_overall_accuracy\", metrics[\"overall_accuracy\"])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters()],\n",
    "                \"lr\": self.learning_rate,\n",
    "            }\n",
    "        ]\n",
    "        return torch.optim.AdamW(\n",
    "            param_dicts, lr=self.learning_rate, weight_decay=self.weight_decay\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "\n",
    "    # Define the callbacks of the model\n",
    "    early_stopping_cb = EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "    logger = TensorBoardLogger(save_dir=f\"../reports/logs/trial_{trial.number}\")\n",
    "\n",
    "    # Create the model\n",
    "    model = SegFormer(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Train the model\n",
    "    trainer = Trainer(\n",
    "        logger=logger,\n",
    "        devices=1,\n",
    "        accelerator=ACCELERATOR,\n",
    "        max_epochs=5,\n",
    "        gradient_clip_val=0.1,\n",
    "        log_every_n_steps=5,\n",
    "        callbacks=[early_stopping_cb],\n",
    "    )\n",
    "    trainer.fit(model, train_sample_dataloader, test_sample_dataloader)\n",
    "    return trainer.callback_metrics[\"val_mean_iou\"].item()\n",
    "\n",
    "def get_best_params(force_tune: bool = False) -> dict:\n",
    "    config_file = \"../config/segformer_hp.yaml\"\n",
    "\n",
    "    if os.path.exists(config_file) and force_tune:\n",
    "        os.remove(config_file)\n",
    "        pruner = optuna.pruners.MedianPruner()\n",
    "        study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "\n",
    "        study.optimize(objective, n_trials=25)\n",
    "        best_params = study.best_params\n",
    "        with open(config_file, \"w\") as file:\n",
    "            yaml.dump(best_params, file)\n",
    "    elif os.path.exists(config_file):\n",
    "        with open(config_file, \"r\") as file:\n",
    "            best_params = yaml.safe_load(file)\n",
    "    else:\n",
    "        pruner = optuna.pruners.MedianPruner()\n",
    "        study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "\n",
    "        study.optimize(objective, n_trials=25)\n",
    "        best_params = study.best_params\n",
    "        with open(config_file, \"w\") as file:\n",
    "            yaml.dump(best_params, file)\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = get_best_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmenendezg/Developer/Projects/fluorescent-neuronal-cells/.venv/lib/python3.11/site-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/Users/mmenendezg/Developer/Projects/fluorescent-neuronal-cells/.venv/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = SegFormer(\n",
    "    learning_rate=best_params[\"learning_rate\"], weight_decay=best_params[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# Define the callbacks of the model\n",
    "early_stopping_cb = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "model_checkpoint_cb = ModelCheckpoint(save_top_k=1, dirpath=f\"../models/SegFormer\", monitor=\"val_loss\")\n",
    "logger = TensorBoardLogger(save_dir=f\"../reports/logs/SegFormer\")\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    devices=1,\n",
    "    accelerator=ACCELERATOR,\n",
    "    precision=16,\n",
    "    max_epochs=100,\n",
    "    gradient_clip_val=0.1,\n",
    "    log_every_n_steps=5,\n",
    "    callbacks=[early_stopping_cb, model_checkpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: ../reports/logs/SegFormer/lightning_logs\n",
      "/Users/mmenendezg/Developer/Projects/fluorescent-neuronal-cells/.venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /Users/mmenendezg/Developer/Projects/fluorescent-neuronal-cells/models/SegFormer exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name  | Type                             | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model | SegformerForSemanticSegmentation | 3.8 M \n",
      "-----------------------------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.011    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444d3527b6b44869a2cdab810647112f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmenendezg/Developer/Projects/fluorescent-neuronal-cells/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/mmenendezg/Developer/Projects/fluorescent-neuronal-cells/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/mmenendezg/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  iou = total_area_intersect / total_area_union\n",
      "/Users/mmenendezg/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = total_area_intersect / total_area_label\n",
      "/Users/mmenendezg/Developer/Projects/fluorescent-neuronal-cells/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50854ceda5224ca781f92d696fb0de52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
